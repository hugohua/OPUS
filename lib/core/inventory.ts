import { redis as connection } from '@/lib/queue/connection';
import { BriefingPayload, SessionMode } from '@/types/briefing';
import { createLogger } from '@/lib/logger';
import { inventoryQueue } from '@/lib/queue';
import { auditInventoryEvent } from '@/lib/services/audit-service';

const log = createLogger('lib:inventory');

// Redis Key Generator
const keys = {
    // Redis Key å¿…é¡»ä¿æŒ Mode ç²’åº¦ï¼Œå› ä¸ºä¸åŒ Mode ç”Ÿæˆçš„å†…å®¹ç»“æ„ä¸åŒ
    // e.g., SYNTAX vs BLITZ vs AUDIO
    drillList: (userId: string, mode: string, vocabId: number | string) =>
        `user:${userId}:mode:${mode}:vocab:${vocabId}:drills`,
    replenishBuffer: 'buffer:replenish_drills',
    stats: (userId: string) => `user:${userId}:inventory:stats`,
};

/**
 * æ ¸å¿ƒåº“å­˜æ¨¡å— (Schedule-Driven)
 * è´Ÿè´£ç®¡ç†å•è¯é¢—ç²’åº¦çš„å¼¹è¯åº“
 */
export const inventory = {
    /**
     * å°†ç”Ÿæˆçš„ Drill æ¨å…¥åº“å­˜
     */
    async pushDrill(userId: string, mode: string, vocabId: number | string, drill: BriefingPayload) {
        // [Fix] Capacity Guard - Prevent Overflow
        const stats: any = await this.getInventoryStats(userId);
        const currentCount = stats[mode] || 0;
        const capacity = await this.getCapacity(mode);

        if (currentCount >= capacity) {
            log.warn({ userId, mode, vocabId, currentCount, capacity }, 'â›” pushDrill blocked - inventory full');
            auditInventoryEvent(userId, 'ADD', mode, { currentCount, capacity, source: 'auto' });
            return; // Early exit
        }

        const key = keys.drillList(userId, mode, vocabId);

        // Multi-exec for atomicity
        const pipeline = connection.pipeline();
        pipeline.rpush(key, JSON.stringify(drill));
        pipeline.hincrby(keys.stats(userId), mode, 1);
        await pipeline.exec();

        log.info({ userId, mode, vocabId }, 'Drill pushed to inventory');
    },

    /**
     * æ‰¹é‡æ¶ˆè´¹ Drill (Only for Mixed Mode)
     * è§£å†³ N+1 æŸ¥è¯¢é—®é¢˜ï¼Œä¸€æ¬¡æ€§è·å–æ‰€æœ‰åœºæ™¯çš„ Drill
     * 
     * @param userId 
     * @param scenarioGroups { "SYNTAX": [vid1, vid2], "PHRASE": [vid3] }
     */
    async popDrillBatch(userId: string, scenarioGroups: Record<string, number[]>): Promise<Record<number, BriefingPayload>> {
        const pipeline = connection.pipeline();
        const orderedRequests: { mode: string, vocabId: number }[] = [];

        // 1. æ„å»º Pipeline
        for (const [mode, vids] of Object.entries(scenarioGroups)) {
            for (const vid of vids) {
                const key = keys.drillList(userId, mode, vid);
                pipeline.lpop(key);
                orderedRequests.push({ mode, vocabId: vid });
            }
        }

        if (orderedRequests.length === 0) return {};

        // 2. æ‰§è¡Œæ‰¹é‡æ“ä½œ
        const results = await pipeline.exec();
        const resultMap: Record<number, BriefingPayload> = {};
        const replenishTriggers: Promise<void>[] = [];

        // 3. å¤„ç†ç»“æœ
        orderedRequests.forEach((req, index) => {
            const [err, data] = results?.[index] || [null, null];

            if (!err && data) {
                // å¦‚æœæˆåŠŸè·å–æ•°æ®
                resultMap[req.vocabId] = JSON.parse(data as string);

                // å¼‚æ­¥é€’å‡ç»Ÿè®¡ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
                connection.hincrby(keys.stats(userId), req.mode, -1).catch(e =>
                    log.error({ error: e, ...req }, 'Stats decrement failed')
                );
            }

            // æ£€æŸ¥æ°´ä½ï¼ˆæ— è®ºæ˜¯å¦ Cache Missï¼Œéƒ½è¦æ£€æŸ¥ï¼Œä¿æŒåº“å­˜å¥åº·ï¼‰
            replenishTriggers.push(
                this.checkAndTriggerReplenish(userId, req.mode, req.vocabId).catch(e =>
                    log.error({ error: e.message, ...req }, 'Batch replenish trigger failed')
                )
            );
        });

        // å¼‚æ­¥ç­‰å¾…æ‰€æœ‰æ°´ä½æ£€æŸ¥è§¦å‘ï¼ˆä¸é˜»å¡è¿”å›ï¼‰
        Promise.all(replenishTriggers).catch(e => log.error(e));

        return resultMap;
    },

    /**
     * æ¶ˆè´¹ä¸€ä¸ª Drill (åŸå­æ“ä½œ)
     * Side Effect: å¦‚æœåº“å­˜æ°´ä½ä½ (<2)ï¼Œè§¦å‘åå°è¡¥å……
     */
    async popDrill(userId: string, mode: string, vocabId: number | string): Promise<BriefingPayload | null> {
        const key = keys.drillList(userId, mode, vocabId);

        // 1. Pop content
        const results = await connection.multi()
            .lpop(key)
            .exec();

        const data = results?.[0]?.[1] as string | null;

        // If we popped something, decrement stats
        if (data) {
            await connection.hincrby(keys.stats(userId), mode, -1);
        }

        // 2. Check remaining length (Async check)
        this.checkAndTriggerReplenish(userId, mode, vocabId).catch(err => {
            log.error({ error: err.message, userId, mode, vocabId }, 'Failed to trigger replenish');
        });

        if (!data) return null;
        return JSON.parse(data);
    },

    /**
     * æ£€æŸ¥åº“å­˜æ°´ä½å¹¶è§¦å‘è¡¥å……
     */
    async checkAndTriggerReplenish(userId: string, mode: string, vocabId: number | string) {
        const key = keys.drillList(userId, mode, vocabId);
        const len = await connection.llen(key);

        // [P1] LOW_WATERMARK = 3 (åŸä¸º 2)
        if (len < 3) {
            log.info({ userId, mode, vocabId, len }, 'ğŸ“‰ Low inventory detected. Buffering for replenishment.');
            // Add to buffer for Batch Aggregation (Plan C)
            await this.addToBuffer(userId, mode, vocabId);

            // Trigger check immediately
            await this.checkBufferAndFlush();
        }
    },

    /**
     * å°† ç¼ºè´§ID åŠ å…¥ç¼“å†²åŒº
     * Format: "userId:mode:vocabId"
     */
    async addToBuffer(userId: string, mode: string, vocabId: number | string) {
        const item = `${userId}:${mode}:${vocabId}`;
        await connection.sadd(keys.replenishBuffer, item);
    },

    /**
     * æ£€æŸ¥ç¼“å†²åŒºå¹¶ Flush (å¦‚æ»¡è¶³é˜ˆå€¼)
     */
    async checkBufferAndFlush() {
        const count = await connection.scard(keys.replenishBuffer);

        // Threshold = 5
        if (count >= 5) {
            await this.flushBuffer();
        }
    },

    /**
     * å¼ºåˆ¶ Flush ç¼“å†²åŒº (ç”Ÿæˆ Batch Job)
     */
    async flushBuffer() {
        // Pop 10 items to process
        const items = await connection.spop(keys.replenishBuffer, 10);
        if (items.length === 0) return;

        // Group by User + Mode
        const groupedJobs: Record<string, number[]> = {};

        for (const item of items) {
            const parts = item.split(':');
            // item is uid:mode:vid. But mode might contain chars?
            // Safer parsing: 
            // format: userId:mode:vocabId. 
            // userId is cuid (string), mode is enum, vocabId is int.

            if (parts.length < 3) continue;

            const vocabId = parseInt(parts.pop()!);
            const mode = parts.pop()!;
            const userId = parts.join(':'); // remaining part is userId

            const jobKey = `${userId}:${mode}`;

            if (!groupedJobs[jobKey]) groupedJobs[jobKey] = [];
            groupedJobs[jobKey].push(vocabId);
        }

        // Enqueue Batch Jobs (Plan C)
        for (const [key, vids] of Object.entries(groupedJobs)) {
            const [uid, mode] = key.split(':');

            await inventoryQueue.add('replenish_batch', {
                userId: uid,
                mode: mode as SessionMode,
                vocabIds: vids,
                correlationId: `batch-replenish-${Date.now()}`
            }, {
                priority: 5 // Low priority for Plan C
            });

            log.info({ userId: uid, mode, count: vids.length }, 'ğŸ“¦ Batch replenishment job enqueued');
        }
    },

    /**
     * è§¦å‘æ€¥æ•‘ä»»åŠ¡ (Plan B)
     */
    async triggerEmergency(userId: string, mode: string, vocabId: number | string) {
        // [P1] Job Deduplication: ä½¿ç”¨ç¡®å®šæ€§ Job ID é˜²æ­¢é‡å¤å…¥é˜Ÿ
        const jobId = `replenish:${userId}:${mode}:${vocabId}`;

        await inventoryQueue.add('replenish_one', {
            userId,
            mode: mode as SessionMode,
            vocabId: Number(vocabId),
            correlationId: `emergency-${vocabId}-${Date.now()}`
        }, {
            jobId, // BullMQ ä¼šå¿½ç•¥é‡å¤ jobId
            priority: 1 // High Priority
        });
        log.info({ userId, mode, vocabId, jobId }, 'ğŸš‘ Emergency replenishment triggered');
    },

    /**
     * è§¦å‘æ‰¹é‡æ€¥æ•‘ä»»åŠ¡ (Plan B in Batch)
     * ç”¨äºå†·å¯åŠ¨æ—¶ï¼Œä¸€æ¬¡æ€§è¡¥å……å¤šä¸ªç¼ºè´§å•è¯ï¼Œé¿å…å‘é€å¤šä¸ªå•ç‹¬çš„æ€¥æ•‘åŒ…
     */
    async triggerBatchEmergency(userId: string, mode: string, vocabIds: number[]) {
        if (vocabIds.length === 0) return;

        // [P1] Job Deduplication: ä½¿ç”¨æ—¶é—´çª—å£ï¼ˆåˆ†é’Ÿçº§ï¼‰é˜²æ­¢çŸ­æ—¶é—´å†…é‡å¤æäº¤
        const timeWindow = Math.floor(Date.now() / 60000); // 1åˆ†é’Ÿçª—å£
        const jobId = `replenish-batch:${userId}:${mode}:${timeWindow}`;

        await inventoryQueue.add('replenish_batch', {
            userId,
            mode: mode as SessionMode,
            vocabIds,
            correlationId: `batch-emergency-${Date.now()}`
        }, {
            jobId, // BullMQ ä¼šå¿½ç•¥é‡å¤ jobId
            priority: 1 // High Priority (Same as Emergency)
        });
        log.info({ userId, mode, count: vocabIds.length, jobId }, 'ğŸš‘ğŸ“¦ Batch Emergency replenishment triggered');
    },

    /**
     * è·å–åº“å­˜ç»Ÿè®¡
     */
    async getInventoryStats(userId: string) {
        const raw = await connection.hgetall(keys.stats(userId));

        // Convert string values to numbers
        return {
            SYNTAX: parseInt(raw.SYNTAX || '0'),
            PHRASE: parseInt(raw.PHRASE || '0'),
            CHUNKING: parseInt(raw.CHUNKING || '0'),
            AUDIO: parseInt(raw.AUDIO || '0'),
            NUANCE: parseInt(raw.NUANCE || '0'),
            READING: parseInt(raw.READING || '0'),
            BLITZ: parseInt(raw.BLITZ || '0'),
            VISUAL: parseInt(raw.VISUAL || '0'),
            total: Object.values(raw).reduce((a: number, b: string) => a + (parseInt(b) || 0), 0)
        };
    },
    /**
     * getInventoryCounts
     * æ‰¹é‡è·å–æŒ‡å®šå•è¯çš„åº“å­˜æ•°é‡
     */
    async getInventoryCounts(userId: string, mode: string, vocabIds: number[]): Promise<Record<number, number>> {
        if (vocabIds.length === 0) return {};

        const pipeline = connection.pipeline();
        vocabIds.forEach((vid) => {
            pipeline.llen(keys.drillList(userId, mode, vid));
        });

        const results = await pipeline.exec();
        const counts: Record<number, number> = {};

        vocabIds.forEach((vid, index) => {
            const result = results?.[index];
            // result is [error, result]
            const count = result && result[0] === null ? (result[1] as number) : 0;
            counts[vid] = count;
        });

        return counts;
    },

    /**
     * æ£€æŸ¥åº“å­˜æ˜¯å¦å·²æ»¡ (Single Source of Truth)
     * @param userId
     * @param mode
     */
    async isFull(userId: string, mode: string): Promise<boolean> {
        const stats: any = await this.getInventoryStats(userId);
        const currentCount = stats[mode] || 0;
        const capacity = await this.getCapacity(mode);
        return currentCount >= capacity;
    },

    /**
     * è·å–æœ€å¤§å®¹é‡ (Drills)
     */
    async getCapacity(mode: string): Promise<number> {
        const { CACHE_LIMIT_MAP, DRILLS_PER_BATCH } = await import('@/lib/drill-cache');
        // Max Limit = Limit (Batches) * DRILLS_PER_BATCH 
        return (CACHE_LIMIT_MAP[mode as SessionMode] || 5) * DRILLS_PER_BATCH;
    },

    /**
     * æ¸…ç©ºæŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰åº“å­˜
     * @param userId ç”¨æˆ· ID
     * @returns åˆ é™¤çš„ Key æ•°é‡
     */
    async clearAll(userId: string): Promise<number> {
        // 1. Find all inventory keys for this user
        const pattern = `user:${userId}:mode:*:vocab:*:drills`;
        let cursor = '0';
        const keysToDelete: string[] = [];

        do {
            // âœ… é‡å‘½åä¸º foundKeys é¿å…ä¸é¡¶å±‚ keys å¯¹è±¡å†²çª
            const [nextCursor, foundKeys] = await connection.scan(cursor, 'MATCH', pattern, 'COUNT', 100);
            cursor = nextCursor;
            keysToDelete.push(...foundKeys);
        } while (cursor !== '0');

        // 2. Also include stats key (ä½¿ç”¨é¡¶å±‚ keys å¯¹è±¡)
        keysToDelete.push(keys.stats(userId));

        if (keysToDelete.length === 0) {
            log.info({ userId }, 'No inventory keys to delete');
            return 0;
        }

        // 3. Delete in batches to avoid Redis command limits
        let deletedCount = 0;
        const BATCH_SIZE = 100;

        for (let i = 0; i < keysToDelete.length; i += BATCH_SIZE) {
            const batch = keysToDelete.slice(i, i + BATCH_SIZE);
            deletedCount += await connection.del(...batch);
        }

        log.info({ userId, deletedCount, keyCount: keysToDelete.length }, 'ğŸ—‘ï¸ Inventory cleared');

        return deletedCount;
    },

    /**
     * æ¸…ç©ºæŒ‡å®š Mode çš„åº“å­˜
     */
    async clearMode(userId: string, mode: string): Promise<number> {
        // 1. Find keys for this specific mode
        const pattern = `user:${userId}:mode:${mode}:vocab:*:drills`;
        let cursor = '0';
        const keysToDelete: string[] = [];

        do {
            const [nextCursor, foundKeys] = await connection.scan(cursor, 'MATCH', pattern, 'COUNT', 100);
            cursor = nextCursor;
            keysToDelete.push(...foundKeys);
        } while (cursor !== '0');

        if (keysToDelete.length > 0) {
            // Delete in batches
            const BATCH_SIZE = 100;
            for (let i = 0; i < keysToDelete.length; i += BATCH_SIZE) {
                const batch = keysToDelete.slice(i, i + BATCH_SIZE);
                await connection.del(...batch);
            }
        }

        // 2. Reset stats for this mode
        await connection.hset(keys.stats(userId), mode, 0);

        log.info({ userId, mode, deletedCount: keysToDelete.length }, 'ğŸ—‘ï¸ Mode inventory cleared');
        return keysToDelete.length;
    }
};
