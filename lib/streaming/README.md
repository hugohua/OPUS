# Universal SSE Streaming Utility

æ ‡å‡†åŒ– LLM æµå¼å“åº”å¤„ç†å·¥å…·ï¼ŒåŸºäº tuoye é¡¹ç›®çš„æˆåŠŸæ¨¡å¼ã€‚

## æ ¸å¿ƒç»„ä»¶

### `handleOpenAIStream(messages, options)`

ç»Ÿä¸€çš„ OpenAI SDK æµå¼å¤„ç†å°è£…ï¼Œè¿”å›æ ‡å‡† SSE Responseã€‚

**ç‰¹æ€§**:
- âœ… å•ä¾‹ OpenAI å®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨é…ç½® DashScope/OpenAIï¼‰
- âœ… æ ‡å‡† SSE æ ¼å¼ `{type: "content"|"done"|"error", data}`
- âœ… å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—
- âœ… å¯é€‰çš„ `onContent` å’Œ `onComplete` å›è°ƒ

## ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬ç”¨æ³•

```typescript
import { handleOpenAIStream, buildMessages } from '@/lib/streaming/sse';

export async function POST(req: Request) {
    const { prompt } = await req.json();
    
    const messages = buildMessages(prompt, "You are a helpful assistant");
    
    return handleOpenAIStream(messages, {
        model: "qwen-plus",
        temperature: 0.7
    });
}
```

### å®Œæ•´ç¤ºä¾‹ï¼ˆå¸¦å›è°ƒï¼‰

```typescript
import { handleOpenAIStream } from '@/lib/streaming/sse';

export async function POST(req: Request) {
    const messages = [
        { role: "system", content: "System prompt..." },
        { role: "user", content: "User input..." }
    ];
    
    return handleOpenAIStream(messages, {
        model: process.env.QWEN_MODEL_NAME || "qwen-plus",
        temperature: 0.7,
        errorContext: "My Feature",
        onContent: (chunk) => {
            // å¯é€‰: è®°å½•æŒ‡æ ‡
            console.log("Received chunk:", chunk.length);
        },
        onComplete: (fullText) => {
            // å¯é€‰: ä¿å­˜åˆ°æ•°æ®åº“ã€è§¦å‘åå¤„ç†ç­‰
            console.log("Generation completed:", fullText.length, "chars");
        }
    });
}
```

## å‰ç«¯æ¶ˆè´¹ç¤ºä¾‹

```typescript
const response = await fetch('/api/my-endpoint', {
    method: 'POST',
    body: JSON.stringify({ prompt: "Hello" })
});

const reader = response.body?.getReader();
const decoder = new TextDecoder();
let buffer = '';

while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    buffer += decoder.decode(value, { stream: true });
    const lines = buffer.split('\n');
    buffer = lines.pop() || '';

    for (const line of lines) {
        if (line.startsWith('data: ')) {
            const data = JSON.parse(line.slice(6));
            
            switch (data.type) {
                case 'content':
                    // ç´¯ç§¯æ˜¾ç¤º
                    setText(prev => prev + data.data);
                    break;
                case 'done':
                    console.log('Stream completed');
                    break;
                case 'error':
                    console.error('Server error:', data.error);
                    break;
            }
        }
    }
}
```

## é¡¹ç›®ä¸­çš„åº”ç”¨

### å·²é‡æ„
- âœ… **WeaverLab** (`/api/weaver/generate`) - L3 æ•…äº‹ç”Ÿæˆ

### å¾…è¿ç§»
- ğŸ”„ æœªæ¥å¯è¿ç§»çš„æµå¼åœºæ™¯ï¼ˆå¦‚éœ€è¦ï¼‰:
  - L2 SmartContent æ‰¹é‡ç”Ÿæˆ
  - å¯¹è¯å¼äº¤äº’åŠŸèƒ½

## é…ç½®

å·¥å…·è‡ªåŠ¨è¯»å–ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰:

```env
# Unified Config
OPENAI_API_KEY=sk-xxx
OPENAI_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
AI_MODEL_NAME=qwen-plus
```

## è°ƒè¯•

æ‰€æœ‰é”™è¯¯ä¼šï¼š
1. æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆå¸¦ `errorContext`ï¼‰
2. é€šè¿‡ SSE å‘é€åˆ°å‰ç«¯ `{type: 'error', error: "..."}`

## æœ€ä½³å®è·µ

1. **æ˜ç¡® errorContext**: ä¾¿äºæ—¥å¿—è¿½è¸ª
   ```typescript
   errorContext: "WeaverLab Generation"
   ```

2. **ä½¿ç”¨ onComplete åšåå¤„ç†**: å¦‚ä¿å­˜åˆ°æ•°æ®åº“ã€è§¦å‘é€šçŸ¥
   ```typescript
   onComplete: async (text) => {
       await saveToDatabase(text);
   }
   ```

3. **å‰ç«¯é˜²æŠ–**: é¿å…é«˜é¢‘ setState
   ```typescript
   const debouncedUpdate = debounce(setText, 50);
   ```

## å‚è€ƒ

- è®¾è®¡çµæ„Ÿ: [tuoye/server.js](file:///Users/hugo/github/tuoye/server.js)
- å®é™…åº”ç”¨: [WeaverLab Route](file:///Users/hugo/github/OPUS/app/api/weaver/generate/route.ts)
