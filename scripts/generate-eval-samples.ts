
/**
 * è¯„ä¼°æ ·æœ¬ç”Ÿæˆè„šæœ¬ (Prompt Eval Generator)
 * 
 * åŠŸèƒ½ï¼š
 *   1. ä»Žæ•°æ®åº“èŽ·å–çœŸå®žè¯æ±‡ (ä¼˜å…ˆ TOEIC æ ¸å¿ƒè¯)ã€‚
 *   2. åˆ†æ‰¹æ¬¡è°ƒç”¨ LLM ç”Ÿæˆ L0/L1/L2 ç­‰çº§çš„ Prompt å’Œ Resultã€‚
 *   3. L0 åŒ…å«ä¸‰ä¸ªå˜ä½“ï¼šSyntax, Phrase, Blitzã€‚
 *   4. ç”ŸæˆåŒ…å« "System Prompt", "User Prompt", "Result" åŠ "äººå·¥è¯„ä¼°æŒ‡ä»¤" çš„ Markdown æŠ¥å‘Šã€‚
 * 
 * ä½¿ç”¨æ–¹æ³•ï¼š
 *   npx tsx scripts/generate-eval-samples.ts [options]
 * 
 * å‚æ•°ï¼š
 *   --limit=N    å•æ¬¡ç”Ÿæˆçš„å•è¯æ•°é‡ (é»˜è®¤: 5)ã€‚
 *   --level=N    æŒ‡å®šç”Ÿæˆç­‰çº§: 0, 1, 2, all (é»˜è®¤: all)ã€‚
 *   --variant=V  æŒ‡å®šç­‰çº§ä¸‹çš„å…·ä½“å˜ä½“ (é»˜è®¤: all):
 *                - syntax, phrase, blitz (L0)
 *                - chunking (L1)
 *                - context (L2)
 *   --model=TYPE æŒ‡å®šä½¿ç”¨çš„ AI æ¨¡åž‹åœºæ™¯:
 *                - default: ä½¿ç”¨æ ‡å‡†æ¨¡åž‹ (å¦‚ qwen-plus)
 *                - etl:     ä½¿ç”¨ ETL ä¸“ç”¨æ¨¡åž‹ (å¦‚ gemini-flash)
 *   --out=DIR    è¾“å‡ºç›®å½• (é»˜è®¤: reports)ã€‚
 * 
 * ç¤ºä¾‹ï¼š
 *   # 1. åŸºç¡€ç”¨æ³• (é»˜è®¤æ¨¡åž‹, L0, 10ä¸ªæ ·æœ¬)
 *   npx tsx scripts/generate-eval-samples.ts --limit=10 --level=0
 *
 *   # 2. ä»…ç”Ÿæˆ L0 çš„ Phrase å˜ä½“
 *   npx tsx scripts/generate-eval-samples.ts --limit=10 --level=0 --variant=phrase
 * 
 *   # 3. ä½¿ç”¨ ETL æ¨¡åž‹ç”Ÿæˆ 5 ä¸ª L1 æ ·æœ¬
 *   npx tsx scripts/generate-eval-samples.ts --limit=5 --level=1 --model=etl
 */
import { PrismaClient } from '@prisma/client';
import { getL0SyntaxBatchPrompt } from '../lib/generators/l0/syntax';
import { getL0PhraseBatchPrompt } from '../lib/generators/l0/phrase';
import { getL0BlitzBatchPrompt } from '../lib/generators/l0/blitz';
import { getL1ChunkingBatchPrompt } from '../lib/generators/l1/chunking';
import { getL2ContextBatchPrompt } from '../lib/generators/l2/context';
import { generateText } from 'ai';
import { getAIModel, AIScenario } from '../lib/ai/client';
import fs from 'fs/promises';
import path from 'path';

// Parse args
const args = process.argv.slice(2);
const limitArg = args.find(a => a.startsWith('--limit='));
const limit = limitArg ? parseInt(limitArg.split('=')[1]) : 5;
const outDirArg = args.find(a => a.startsWith('--out='));
const outDir = outDirArg ? outDirArg.split('=')[1] : 'reports';
const levelArg = args.find(a => a.startsWith('--level='));
const level = levelArg ? levelArg.split('=')[1] : 'all'; // '0', '1', '2', 'all'
const modelArg = args.find(a => a.startsWith('--model='));
const modelScenario = (modelArg && modelArg.split('=')[1] === 'etl') ? 'etl' : 'default';
const variantArg = args.find(a => a.startsWith('--variant='));
const variant = variantArg ? variantArg.split('=')[1].toLowerCase() : 'all'; // 'syntax', 'phrase', 'blitz', 'chunking', 'context', 'all'

const prisma = new PrismaClient();

async function main() {
    await fs.mkdir(outDir, { recursive: true });

    console.log(`Generating samples for level: ${level}, limit: ${limit}`);
    console.log(`Selected Model Scenario: ${modelScenario.toUpperCase()}`);
    if (variant !== 'all') console.log(`Selected Variant: ${variant.toUpperCase()}`);

    // 1. Fetch Words (TOEIC core first)
    let words = await prisma.vocab.findMany({
        where: { is_toeic_core: true },
        take: limit,
    });

    if (words.length < limit) {
        console.log(`Found only ${words.length} TOEIC core words. Fetching more...`);
        const needed = limit - words.length;
        const anyWords = await prisma.vocab.findMany({
            where: { id: { notIn: words.map(w => w.id) } },
            take: needed
        });
        words.push(...anyWords);
    }

    // Use 'default' or 'etl' scenario based on args
    const { model, modelName } = getAIModel(modelScenario);
    console.log(`--------------------------------------------------`);
    console.log(`ðŸš€ USING AI MODEL: [${modelName}]`);
    console.log(`--------------------------------------------------`);

    // Prepare inputs for each level
    const l0SyntaxInputs: any[] = [];
    const l0PhraseInputs: any[] = [];
    const l0BlitzInputs: any[] = [];
    const l1Inputs: any[] = [];
    const l2Inputs: any[] = [];

    for (const word of words) {
        const family = (word.word_family as Record<string, string>) || {};

        // L0 Variants Input
        if (level === 'all' || level === '0') {
            const randomWords = await prisma.vocab.findMany({
                take: 3, // Increased for phrase modifiers
                skip: Math.floor(Math.random() * 100),
                where: { id: { not: word.id } }
            });
            const randomList = randomWords.map(w => w.word);

            // 1. Syntax (S-V-O)
            if (variant === 'all' || variant === 'syntax') {
                l0SyntaxInputs.push({
                    targetWord: word.word,
                    meaning: word.definition_cn || 'No definition',
                    contextWords: randomList.slice(0, 2),
                    wordFamily: family
                });
            }

            // 2. Phrase (1+N modifiers)
            if (variant === 'all' || variant === 'phrase') {
                l0PhraseInputs.push({
                    targetWord: word.word,
                    modifiers: randomList.slice(0, 1) // Mock modifiers
                });
            }

            // 3. Blitz (Collocations)
            if (variant === 'all' || variant === 'blitz') {
                l0BlitzInputs.push({
                    targetWord: word.word,
                    meaning: word.definition_cn || '',
                    collocations: randomList // Mock collocations
                });
            }
        }

        // L1 Input
        if (level === 'all' || level === '1') {
            if (variant === 'all' || variant === 'chunking') {
                if (word.commonExample) {
                    l1Inputs.push({
                        sentence: word.commonExample,
                        targetWord: word.word
                    });
                }
            }
        }

        // L2 Input
        if (level === 'all' || level === '2') {
            if (variant === 'all' || variant === 'context') {
                const randomWords = await prisma.vocab.findMany({
                    take: 3,
                    skip: Math.floor(Math.random() * 100),
                    where: { id: { not: word.id } }
                });
                l2Inputs.push({
                    targetWord: word.word,
                    contextWords: randomWords.map(w => w.word)
                });
            }
        }
    }

    // Execute Batch Generation

    // L0 Variants
    if (l0SyntaxInputs.length > 0) {
        const prompts = getL0SyntaxBatchPrompt(l0SyntaxInputs);
        await runBatch('0-Syntax', prompts, model);
    }
    if (l0PhraseInputs.length > 0) {
        const prompts = getL0PhraseBatchPrompt(l0PhraseInputs);
        await runBatch('0-Phrase', prompts, model);
    }
    if (l0BlitzInputs.length > 0) {
        const prompts = getL0BlitzBatchPrompt(l0BlitzInputs);
        await runBatch('0-Blitz', prompts, model);
    }

    if (l1Inputs.length > 0) {
        const prompts = getL1ChunkingBatchPrompt(l1Inputs);
        await runBatch('1', prompts, model);
    }

    if (l2Inputs.length > 0) {
        const prompts = getL2ContextBatchPrompt(l2Inputs);
        await runBatch('2', prompts, model);
    }
}

// Meta-Eval Prompt Template (Used as footer for manual review)
const META_EVAL_TEMPLATE = `
---
# ðŸ’¡ Manual Evaluation Instruction

Please copy the content above and send it to an LLM with the following prompt:

"""
# Role
ä½ æ˜¯ä¸€ä½ç²¾é€š Prompt Engineering çš„ä¸“å®¶ï¼Œæ“…é•¿ä¼˜åŒ– LLM çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›å’Œå†…å®¹ç”Ÿæˆè´¨é‡ã€‚

# Objective
è¯„ä¼°ç”¨æˆ·æä¾›çš„ System Promptã€User Prompt ä»¥åŠ LLM ç”Ÿæˆçš„ Resultã€‚
è¯·åˆ†æž System Prompt æ˜¯å¦æœ€ä¼˜ï¼Œç”Ÿæˆçš„å†…å®¹æ˜¯å¦å®Œå…¨ç¬¦åˆçº¦æŸï¼Œå¹¶ç»™å‡ºä¼˜åŒ–å»ºè®®ã€‚

# Output Format (Markdown)
## è¯„åˆ† (1-10åˆ†)
ç»™å‡ºç»¼åˆè¯„åˆ†ã€‚

## é—®é¢˜åˆ†æž
æŒ‡å‡ºç”Ÿæˆå†…å®¹ä¸­å­˜åœ¨çš„é—®é¢˜ï¼ˆå¦‚æœªéµå¾ªçš„çº¦æŸã€é€»è¾‘æ¼æ´žã€æ ¼å¼é”™è¯¯ç­‰ï¼‰ã€‚

## ä¼˜åŒ–å»ºè®®
é’ˆå¯¹ System Prompt ç»™å‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®ï¼ˆä¸­æ–‡ï¼‰ï¼Œå¦‚æžœæ˜¯ Prompt ç»“æž„é—®é¢˜ï¼Œè¯·æä¾›ä¼˜åŒ–åŽçš„ Prompt ç‰‡æ®µã€‚
"""
`.trim();

async function runBatch(lvl: string, prompts: { system: string, user: string }, model: any) {
    console.log(`Processing Level ${lvl} Batch (${limit} items)...`);
    try {
        // Step 1: Generate Drill Content
        const result = await generateText({
            model,
            system: prompts.system,
            prompt: prompts.user,
        });

        const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
        const filename = `Eval_L${lvl}_${timestamp}.md`;
        const filePath = path.join(outDir, filename);

        const content = `System Prompt:
${prompts.system}

User Prompt:
${prompts.user}

Result:
${result.text}

${META_EVAL_TEMPLATE}
`;

        await fs.writeFile(filePath, content);
        console.log(`Saved Report: ${filename}`);

    } catch (e) {
        console.error(`Failed Batch L${lvl}`, e);
    }
}

main().catch(e => {
    console.error(e);
    process.exit(1);
});
