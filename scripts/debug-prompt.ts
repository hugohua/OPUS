/**
 * Prompt è°ƒè¯•å·¥å…· (Prompt Debugging Tool)
 * ==========================================
 * 
 * ç›®æ ‡: ä½¿ç”¨çœŸå®æ•°æ®åº“æ•°æ®éªŒè¯ System/User Promptï¼Œå¹¶æ”¯æŒæ‰¹é‡ç”Ÿæˆæµ‹è¯•æŠ¥å‘Šã€‚
 * 
 * æ ¸å¿ƒåŠŸèƒ½:
 *   1. çœŸå®æ•°æ®: è¿æ¥ç”Ÿäº§ DB è·å–å®Œæ•´ä¸Šä¸‹æ–‡ (æ­é…è¯ã€è¯æ—ç­‰)ã€‚
 *   2. æ™ºèƒ½é€‰è¯: æ ¹æ®ç”Ÿæˆå™¨ç±»å‹åº”ç”¨ä¸åŒçš„é€‰è¯ç­–ç•¥ï¼Œæ¨¡æ‹Ÿç”Ÿäº§ç¯å¢ƒã€‚
 *   3. æ•°æ®æ ¡éªŒ: ç¡®ä¿è¾“å…¥æ•°æ®æœ‰æ•ˆï¼Œæ— æ•ˆæ—¶ä½¿ç”¨ Pivot (å…œåº•)ã€‚
 *   4. æ‰¹é‡éªŒè¯: é»˜è®¤éšæœºæŠ½å– N ä¸ªè¯è¿›è¡Œæµ‹è¯•ï¼Œç”Ÿæˆæ ‡å‡†åŒ–çš„ output/*.txt æŠ¥å‘Šã€‚
 *   5. LLM æ‰§è¡Œ: æ”¯æŒ --run å‚æ•°å®é™…è°ƒç”¨ AI ç”Ÿæˆç»“æœã€‚
 * 
 * ä½¿ç”¨æ–¹å¼:
 *   1. æ‰¹é‡ç”Ÿæˆ Prompt æŠ¥å‘Š (ä¸è°ƒç”¨ AI):
 *      npx tsx scripts/debug-prompt.ts -g l0-syntax
 * 
 *   2. æ‰§è¡Œ AI ç”Ÿæˆå¹¶æŸ¥çœ‹ç»“æœ:
 *      npx tsx scripts/debug-prompt.ts -g l2-smart --run -n 5
 *
 *   3. æŒ‡å®š L2 Context Stage:
 *      npx tsx scripts/debug-prompt.ts -g l2-context-script --stage 3
 * 
 * å‚æ•°è¯´æ˜:
 *   -g, --gen     : ç”Ÿæˆå™¨ç±»å‹ (l0-syntax | l0-phrase | l0-blitz | l2-smart | l2-context | l2-context-script | l2-nuance)
 *   -n, --number  : æ‰¹é‡å¤§å° (é»˜è®¤ 10)
 *   -r, --run     : æ‰§è¡Œ AI ç”Ÿæˆ
 *   -s, --scenario: æŒ‡å®šåœºæ™¯ (ä»… L2)
 *   --stage       : L2 Context Script é˜¶æ®µ (1|2|3, é»˜è®¤ 1)
 */

import { Command } from 'commander';
import { select } from '@inquirer/prompts';
import { db } from '@/lib/db';
import { AIService } from '@/lib/ai/core';
import { runEtlJob, EtlBatchResult } from '@/lib/etl/batch-runner';
import { getAIModel } from '@/lib/ai/client';
import {
    L0_SYNTAX_SYSTEM_PROMPT,
    getL0SyntaxUserPrompt,
    getL0SyntaxBatchPrompt,
    SyntaxGeneratorInput
} from '@/lib/generators/l0/syntax';
import {
    L0_PHRASE_SYSTEM_PROMPT,
    getL0PhraseBatchPrompt,
    PhraseGeneratorInput
} from '@/lib/generators/l0/phrase';
import {
    L0_BLITZ_SYSTEM_PROMPT,
    getL0BlitzBatchPrompt,
    BlitzGeneratorInput
} from '@/lib/generators/l0/blitz';
import {
    L2_SMART_CONTENT_SYSTEM_PROMPT,
    buildL2SentenceUserPrompt,
    SmartContentInput,
    L2SentencePayloadSchema
} from '@/lib/generators/l2/smart-content';
import {
    L2_CONTEXT_SYSTEM_PROMPT,
    getL2ContextBatchPrompt,
    ContextGeneratorInput
} from '@/lib/generators/l2/context';
import {
    getL2ContextBatchPrompt as getL2ContextScriptBatchPrompt,
    L2ContextInput as L2ContextScriptInput,
    ContextStage
} from '@/lib/generators/l2/context-script';
import {
    L2_NUANCE_SYSTEM_PROMPT,
    getL2NuanceBatchPrompt
} from '@/lib/generators/l2/nuance';
import {
    buildSyntaxInputSimple,
    buildPhraseInput,
    buildBlitzInputWithTraps
} from '@/lib/generators/input-builders';
import { VocabEntity, CollocationItem } from '@/types/vocab';
import { z } from 'zod';
import chalk from 'chalk';

// åŠ è½½ç¯å¢ƒå˜é‡
try { process.loadEnvFile(); } catch { }

const program = new Command();

// ==========================================
// é€šç”¨ç±»å‹å®šä¹‰
// ==========================================

// VocabItem æ¥å£ç°åœ¨ä½¿ç”¨ @/types/vocab ä¸­çš„ VocabEntity
// ä¿ç•™æ­¤åˆ«åä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼Œæˆ–ç›´æ¥æ›¿æ¢
type VocabItem = VocabEntity;

interface DebugContext {
    adapter: GeneratorAdapter;
    runAI: boolean;
    scenario?: string;
    stage?: ContextStage;
    model: any;
    generatorKey: string;
}

// ==========================================
// æ•°æ®æ ¡éªŒå·¥å…· (Fail-Safe)
// ==========================================

/**
 * æ ¡éªŒå¹¶æå– collocations æ•°ç»„
 * @param raw åŸå§‹ collocations æ•°æ® (å¯èƒ½æ˜¯ null, undefined, object, array)
 * @returns æœ‰æ•ˆçš„ string[] æˆ–ç©ºæ•°ç»„ (å…œåº•)
 */
function extractCollocations(raw: any): string[] {
    if (!raw) return [];

    // å¦‚æœæ˜¯æ•°ç»„
    if (Array.isArray(raw)) {
        return raw
            .map((item: any) => {
                // æ”¯æŒ { text: "xxx" } æˆ– { word: "xxx" } æ ¼å¼
                if (typeof item === 'string') return item;
                if (typeof item === 'object' && item !== null) {
                    return item.text || item.word || item.collocation || null;
                }
                return null;
            })
            .filter((s): s is string => typeof s === 'string' && s.length > 0);
    }

    return [];
}

/**
 * æ ¡éªŒæ•°æ®æ˜¯å¦æ»¡è¶³ç”Ÿæˆå™¨æœ€ä½è¦æ±‚
 * @returns true å¦‚æœæ•°æ®æœ‰æ•ˆï¼Œfalse å¦‚æœåº”è¯¥è·³è¿‡
 */
function validateVocabForGenerator(vocab: VocabItem, generatorKey: string): boolean {
    // åŸºç¡€æ ¡éªŒï¼šå¿…é¡»æœ‰å•è¯å’Œé‡Šä¹‰
    if (!vocab.word || !vocab.definition_cn) {
        return false;
    }

    // éœ€è¦æ­é…è¯çš„ç”Ÿæˆå™¨
    const needsCollocations = ['l0-phrase', 'l0-blitz', 'l2-context', 'l2-context-script'];
    if (needsCollocations.includes(generatorKey)) {
        const cols = extractCollocations(vocab.collocations);
        // è‡³å°‘éœ€è¦ 1 ä¸ªæ­é…è¯
        if (cols.length === 0) {
            return false;
        }
    }

    return true;
}

// ==========================================
// Generator Adapter æ¥å£ & å®ç°
// ==========================================

interface GeneratorAdapter {
    name: string;
    /** æ•°æ®è¦æ±‚æè¿° (ç”¨äºæ—¥å¿—/æŠ¥å‘Š) */
    dataRequirements?: string;
    buildInput: (vocab: VocabItem, extra?: any) => any;
    getPrompts: (input: any) => { system: string; user: string; schema?: z.ZodType<any> };
    getBatchPrompts?: (inputs: any[]) => { system: string; user: string; schema?: z.ZodType<any> };
}

const Adapters: Record<string, GeneratorAdapter> = {
    'l0-syntax': {
        name: 'L0 å¥æ³•è®­ç»ƒ / Syntax Rescue',
        dataRequirements: 'éœ€è¦: é‡Šä¹‰, æ­é…è¯ (å¯é€‰)',
        buildInput: (vocab: VocabItem) => buildSyntaxInputSimple(vocab),
        getPrompts: (input: SyntaxGeneratorInput) => ({
            system: L0_SYNTAX_SYSTEM_PROMPT,
            user: getL0SyntaxUserPrompt(input)
        }),
        getBatchPrompts: (inputs: SyntaxGeneratorInput[]) => {
            return getL0SyntaxBatchPrompt(inputs);
        }
    },
    'l0-phrase': {
        name: 'L0 çŸ­è¯­æ‰©å±• / Phrase Expansion',
        dataRequirements: 'éœ€è¦: é‡Šä¹‰, è‡³å°‘ 1 ä¸ªæ­é…è¯',
        buildInput: (vocab: VocabItem) => buildPhraseInput(vocab),
        getPrompts: (input: PhraseGeneratorInput) => ({
            system: L0_PHRASE_SYSTEM_PROMPT,
            user: JSON.stringify(input)
        }),
        getBatchPrompts: (inputs: PhraseGeneratorInput[]) => {
            return getL0PhraseBatchPrompt(inputs);
        }
    },
    'l0-blitz': {
        name: 'L0 é—ªç”µæˆ˜ / Phrase Blitz',
        dataRequirements: 'éœ€è¦: é‡Šä¹‰, è‡³å°‘ 1 ä¸ªæ­é…è¯',
        buildInput: async (vocab: VocabItem) => await buildBlitzInputWithTraps(vocab),
        getPrompts: (input: BlitzGeneratorInput) => ({
            system: L0_BLITZ_SYSTEM_PROMPT,
            user: JSON.stringify(input)
        }),
        getBatchPrompts: (inputs: BlitzGeneratorInput[]) => {
            return getL0BlitzBatchPrompt(inputs);
        }
    },
    'l2-smart': {
        name: 'L2 æ™ºèƒ½å†…å®¹ / Smart Content',
        dataRequirements: 'éœ€è¦: é‡Šä¹‰',
        buildInput: (vocab: VocabItem, extra?: any) => {
            return {
                word: vocab.word,
                definition: vocab.definition_cn || '(æ— é‡Šä¹‰)',
                scenario: extra?.scenario
            } as SmartContentInput;
        },
        getPrompts: (input: SmartContentInput) => ({
            system: L2_SMART_CONTENT_SYSTEM_PROMPT,
            user: buildL2SentenceUserPrompt(input),
            schema: L2SentencePayloadSchema
        })
    },
    'l2-context': {
        name: 'L2 è¯­å¢ƒå¡«ç©º / Context Cloze',
        dataRequirements: 'éœ€è¦: é‡Šä¹‰, æ­é…è¯',
        buildInput: (vocab: VocabItem) => {
            const contextKeywords = extractCollocations(vocab.collocations).slice(0, 3);

            return {
                targetWord: vocab.word,
                meaning: vocab.definition_cn || '',
                contextKeywords
            } as ContextGeneratorInput;
        },
        getPrompts: (input: ContextGeneratorInput) => ({
            system: L2_CONTEXT_SYSTEM_PROMPT,
            user: `GENERATE 1 CONTEXT DRILL.\n\nINPUT DATA:\n${JSON.stringify([input], null, 2)}`
        }),
        getBatchPrompts: (inputs: ContextGeneratorInput[]) => {
            return getL2ContextBatchPrompt(inputs);
        }
    },
    'l2-context-script': {
        name: 'L2 æƒ…å¢ƒè„šæœ¬ / Gap Fill (Part 5/6/7)',
        dataRequirements: 'éœ€è¦: é‡Šä¹‰, æ­é…è¯, åœºæ™¯ (å¯é€‰)',
        buildInput: (vocab: VocabItem, extra?: any) => {
            const contextWords = extractCollocations(vocab.collocations).slice(0, 3);

            return {
                targetWord: vocab.word,
                meaning: vocab.definition_cn || '',
                contextWords,
                scenario: extra?.scenario,
                stage: extra?.stage || 1
            } as L2ContextScriptInput;
        },
        getPrompts: (input: L2ContextScriptInput) => {
            const res = getL2ContextScriptBatchPrompt([input], input.stage);
            return { system: res.system, user: res.user };
        },
        getBatchPrompts: (inputs: L2ContextScriptInput[]) => {
            const stage = inputs[0]?.stage || 1;
            return getL2ContextScriptBatchPrompt(inputs, stage);
        }
    },
    'l2-nuance': {
        name: 'L2 å•†åŠ¡è¾¨æ / Nuance Discrimination',
        dataRequirements: 'éœ€è¦: é‡Šä¹‰ (ç†æƒ³æƒ…å†µéœ€è¦è¿‘ä¹‰è¯å¯¹)',
        buildInput: (vocab: VocabItem) => {
            // Nuance é€šå¸¸éœ€è¦å¯¹æ¯”è¯ï¼Œä½†åœ¨å•ç‹¬è°ƒè¯•æ—¶åªèƒ½ç”¨å•è¯
            return {
                targetWord: vocab.word,
                meaning: vocab.definition_cn || '',
                word: vocab.word,
                definition_cn: vocab.definition_cn
            };
        },
        getPrompts: (input: any) => ({
            system: L2_NUANCE_SYSTEM_PROMPT,
            user: JSON.stringify([input])
        }),
        getBatchPrompts: (inputs: any[]) => {
            return getL2NuanceBatchPrompt(inputs);
        }
    }
};

// ==========================================
// æ™ºèƒ½é€‰è¯ç­–ç•¥ (æ¨¡æ‹Ÿ OMPS)
// ==========================================

/**
 * æ ¹æ®ç”Ÿæˆå™¨ç±»å‹è·å–ç¬¦åˆæ¡ä»¶çš„è¯æ±‡
 * 
 * ç­–ç•¥:
 *   - é€šç”¨: TOEIC æ ¸å¿ƒè¯, æœ‰é‡Šä¹‰
 *   - L0 Phrase/Blitz: å¿…é¡»æœ‰æ­é…è¯
 *   - L2 Context: å¿…é¡»æœ‰æ­é…è¯
 */
const mapToVocabEntity = (raw: any): VocabEntity => ({
    vocabId: raw.id,
    word: raw.word,
    definition_cn: raw.definition_cn,
    word_family: raw.word_family as Record<string, string>,
    collocations: raw.collocations as CollocationItem[],
    partOfSpeech: raw.partOfSpeech
});

async function fetchBatchForGenerator(
    batchSize: number,
    generatorKey: string
): Promise<VocabItem[]> {
    // åŸºç¡€æŸ¥è¯¢æ¡ä»¶ (æ‰€æœ‰ç”Ÿæˆå™¨éƒ½éœ€è¦)
    const baseWhere = {
        is_toeic_core: true,
        definition_cn: { not: null }  // å¿…é¡»æœ‰é‡Šä¹‰
    };

    // éœ€è¦æ­é…è¯çš„ç”Ÿæˆå™¨: é¢å¤–è¿‡æ»¤
    const needsCollocations = ['l0-phrase', 'l0-blitz', 'l2-context', 'l2-context-script'];

    let whereCondition: any = baseWhere;

    if (needsCollocations.includes(generatorKey)) {
        // Prisma JSON è¿‡æ»¤: collocations ä¸ä¸ºç©ºæ•°ç»„
        // æ³¨æ„: Postgres JSON éœ€è¦ç‰¹æ®Šå¤„ç†
        whereCondition = {
            ...baseWhere,
            // ç®€åŒ–: ä½¿ç”¨ NOT null ä½œä¸ºç¬¬ä¸€å±‚è¿‡æ»¤
            // æ›´ç²¾ç¡®çš„è¿‡æ»¤åœ¨ fetchBatch åç”¨ä»£ç å¤„ç†
            collocations: { not: undefined }
        };
    }

    // è·å–ç¬¦åˆæ¡ä»¶çš„æ€»æ•° (ç”¨äºéšæœºé‡‡æ ·)
    const count = await db.vocab.count({ where: whereCondition });
    if (count === 0) {
        console.log(chalk.yellow(`âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆ ${generatorKey} è¦æ±‚çš„è¯æ±‡`));
        return [];
    }

    // éšæœºåç§»
    const skip = Math.max(0, Math.floor(Math.random() * Math.max(0, count - batchSize * 3)));

    // è·å–å€™é€‰è¯ (å¤šå–ä¸€äº›ç”¨äºè¿‡æ»¤)
    const rawCandidates = await db.vocab.findMany({
        where: whereCondition,
        take: batchSize * 3, // å¤šå–ï¼Œé˜²æ­¢è¿‡æ»¤åä¸è¶³
        skip: skip,
        select: {
            id: true,
            word: true,
            definition_cn: true,
            collocations: true,
            word_family: true,
            partOfSpeech: true // [Fix] Add partOfSpeech
        }
    });

    // è½¬æ¢ä¸º VocabEntity
    const vocabEntities = rawCandidates.map(mapToVocabEntity);

    // åº”ç”¨ç”Ÿæˆå™¨ç‰¹å®šçš„æ ¡éªŒè¿‡æ»¤
    const validItems = vocabEntities.filter(item =>
        validateVocabForGenerator(item, generatorKey)
    );

    // å–æœ€ç»ˆéœ€è¦çš„æ•°é‡
    const result = validItems.slice(0, batchSize);

    console.log(chalk.gray(
        `ğŸ“Š é€‰è¯ç»Ÿè®¡: å€™é€‰ ${rawCandidates.length} â†’ æœ‰æ•ˆ ${validItems.length} â†’ é€‰å– ${result.length}`
    ));

    return result;
}

// ==========================================
// ETL å¤„ç†é€»è¾‘
// ==========================================

async function processBatch(
    items: VocabItem[],
    isDryRun: boolean,
    batchIndex: number,
    context?: DebugContext
): Promise<EtlBatchResult> {
    if (!context) throw new Error("Context missing");
    const { adapter, runAI, scenario, stage } = context;

    // 1. æ„å»ºè¾“å…¥ (å¸¦ Pivot ä¿æŠ¤)
    const inputs = items.map(item => adapter.buildInput(item, { scenario, stage }));

    let systemPrompt = "";
    let userPrompt = "";
    let rawResult = "(AI æœªæ‰§è¡Œ)";

    // 2. ç”Ÿæˆ Prompt (Batch vs å•æ¡èšåˆ)
    if (adapter.getBatchPrompts) {
        // åŸç”Ÿ Batch æ”¯æŒ (å¦‚ L0)
        const prompts = adapter.getBatchPrompts(inputs);
        systemPrompt = prompts.system;
        userPrompt = prompts.user;

        if (runAI) {
            const res = await AIService.generateText({
                mode: 'smart',
                system: systemPrompt,
                prompt: userPrompt
            });
            rawResult = res.text;

            // å°è¯•æ ¼å¼åŒ– JSON
            try {
                rawResult = JSON.stringify(JSON.parse(res.text), null, 2);
            } catch { }
        }
    } else {
        // æ‰‹åŠ¨èšåˆ (å¦‚ L2 Smart)
        const firstPrompts = adapter.getPrompts(inputs[0]);
        systemPrompt = firstPrompts.system;

        // æ‹¼æ¥ User Prompts
        const userPromptsRaw = inputs.map((inp, i) => {
            const p = adapter.getPrompts(inp);
            return `--- è¯æ¡ ${i + 1}: ${items[i].word} ---\n${p.user}`;
        });
        userPrompt = userPromptsRaw.join('\n\n');

        if (runAI) {
            const results = [];
            for (const [i, input] of inputs.entries()) {
                const p = adapter.getPrompts(input);
                let resStr = "";
                try {
                    if (p.schema) {
                        const r = await AIService.generateObject({
                            mode: 'smart',
                            system: p.system,
                            prompt: p.user,
                            schema: p.schema
                        });
                        resStr = JSON.stringify(r.object, null, 2);
                    } else {
                        const r = await AIService.generateText({
                            mode: 'smart',
                            system: p.system,
                            prompt: p.user
                        });
                        resStr = r.text;
                    }
                } catch (e: any) {
                    resStr = `é”™è¯¯: ${e.message}`;
                }
                results.push(`--- ç»“æœ ${i + 1}: ${items[i].word} ---\n${resStr}`);
            }
            rawResult = results.join('\n\n');
        } else {
            rawResult = "(AI æœªæ‰§è¡Œ - æ‰¹é‡æ¨¡å¼)";
        }
    }

    return {
        successCount: items.length,
        failedCount: 0,
        debugInfo: {
            systemPrompt: systemPrompt,
            userPrompt: userPrompt,
            rawResult: rawResult,
            batchId: `batch_${batchIndex}`
        }
    };
}

// ==========================================
// ä¸»ç¨‹åº
// ==========================================

async function main() {
    program
        .name('debug-prompt')
        .description('ä½¿ç”¨çœŸå® DB æ•°æ®è°ƒè¯• LLM Prompt (ETL æ¨¡å¼)')
        .option('-g, --gen <type>', 'ç”Ÿæˆå™¨ç±»å‹')
        .option('-n, --number <count>', 'å¤„ç†æ•°é‡', '10')
        .option('-r, --run', 'æ‰§è¡Œ AI ç”Ÿæˆ', false)
        .option('-s, --scenario <scenario>', 'L2 åœºæ™¯')
        .option('--stage <stage>', 'L2 Context Script é˜¶æ®µ (1|2|3)', '1');

    program.parse(process.argv);
    const options = program.opts();

    // é€‰æ‹©ç”Ÿæˆå™¨
    let genKey = options.gen;
    if (!genKey) {
        genKey = await select({
            message: 'é€‰æ‹©ç”Ÿæˆå™¨:',
            choices: Object.entries(Adapters).map(([key, adapter]) => ({
                name: `${adapter.name}`,
                value: key,
                description: adapter.dataRequirements
            }))
        });
    }

    const adapter = Adapters[genKey];
    if (!adapter) {
        console.error(chalk.red(`âŒ æœªçŸ¥ç”Ÿæˆå™¨: ${genKey}`));
        process.exit(1);
    }

    console.log(chalk.blue(`ğŸš€ ç”Ÿæˆå™¨: ${adapter.name}`));
    if (adapter.dataRequirements) {
        console.log(chalk.gray(`ğŸ“‹ æ•°æ®è¦æ±‚: ${adapter.dataRequirements}`));
    }

    const { model, modelName } = getAIModel('etl');
    const stage = parseInt(options.stage) as ContextStage;

    // è¿è¡Œ ETL
    await runEtlJob<VocabItem, DebugContext>({
        jobName: `debug-${genKey}`,
        tier: 'free',
        isDryRun: true,
        fetchBatch: async (size) => fetchBatchForGenerator(size, genKey),
        processBatch,
        context: {
            adapter,
            runAI: options.run,
            scenario: options.scenario,
            stage,
            model,
            generatorKey: genKey
        }
    });
}

main()
    .catch(console.error)
    .finally(() => db.$disconnect());
